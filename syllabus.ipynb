{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module-1Introduction10 hours\n",
    "### Introduction:\n",
    "\n",
    "    Well posed learning problems \n",
    "    Designing a Learning system\n",
    "    Perspective and Issues in Machine Learning.\n",
    "\n",
    " \n",
    "\n",
    "### Concept Learning:\n",
    "\n",
    "    Concept learning task, \n",
    "    Concept learning as search, \n",
    "    Find-S algorithm, \n",
    "    Version space, \n",
    "    Candidate Elimination algorithm, \n",
    "    Inductive Bias.\n",
    "\n",
    "  ### Text Book1, Sections: 1.1 – 1.3, 2.1-2.5, 2.7\n",
    "\n",
    "\n",
    "\n",
    "# Module-2Decision Tree Learning\n",
    "\n",
    "### Decision Tree Learning:\n",
    "\n",
    "    Decision tree representation, \n",
    "    Appropriate problems for decision tree learning, \n",
    "    Basic decision tree learning algorithm, \n",
    "    hypothesis space search in decision tree learning,\n",
    "    Inductive bias in decision tree learning, \n",
    "    Issues in decision tree learning.\n",
    "\n",
    "### Text Book1, Sections: 3.1-3.7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Module-3  Artificial Neural Networks \n",
    "\n",
    "### Artificial Neural Networks:\n",
    "\n",
    "    Introduction, \n",
    "    Neural Network representation, \n",
    "    Appropriate problems, \n",
    "    Perceptrons, \n",
    "    Backpropagation algorithm.\n",
    "\n",
    "### Text book 1, Sections: 4.1 – 4.6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Module-4  Bayesian Learning10 hours\n",
    "\n",
    "### Bayesian Learning:\n",
    "\n",
    "    Introduction, \n",
    "    Bayes theorem, \n",
    "    Bayes theorem and concept learning,\n",
    "    ML and LS error hypothesis, \n",
    "    ML for predicting probabilities, \n",
    "    MDL principle, \n",
    "    Naive Bayes classifier, \n",
    "    Bayesian belief networks, \n",
    "    EM algorithm\n",
    "\n",
    "### Text book 1, Sections: 6.1 – 6.6, 6.9, 6.11, 6.12\n",
    "\n",
    "\n",
    "\n",
    "# Module-5  Evaluating Hypothesis \n",
    "\n",
    "### Evaluating Hypothesis:\n",
    "\n",
    "    Motivation, \n",
    "    Estimating hypothesis accuracy, \n",
    "    Basics of sampling theorem, \n",
    "    General approach for deriving confidence intervals, \n",
    "    Difference in error of two hypothesis, \n",
    "    Comparing learning algorithms.\n",
    "\n",
    " \n",
    "\n",
    "### Instance Based Learning:\n",
    "\n",
    "    Introduction, \n",
    "    k-nearest neighbor learning, \n",
    "    locally weighted regression, \n",
    "    radial basis function, cased-based reasoning,\n",
    "\n",
    "\n",
    "\n",
    "### Reinforcement Learning:\n",
    "\n",
    "    Introduction, \n",
    "    Learning Task, \n",
    "    Q Learning\n",
    "\n",
    "### Text book 1, Sections: 5.1-5.6, 8.1-8.5, 13.1-13.3\n",
    "\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Course Outcomes:\n",
    "\n",
    "After studying this course, students will be able to\n",
    "\n",
    "Recall the problems for machine learning. And select the either supervised, unsupersvised or reinforcement learning.\n",
    "Understand theory of probability and statistics related to machine learning\n",
    "Illustrate concept learning, ANN, Bayes classifier, k nearest neighbor, Q,\n",
    " \n",
    "\n",
    "Question paper pattern:\n",
    "\n",
    "The question paper will have ten questions.\n",
    "There will be 2 questions from each module.\n",
    "Each question will have questions covering all the topics under a module.\n",
    "The students will have to answer 5 full questions, selecting one full question from each module.\n",
    " \n",
    "\n",
    "Text Books:\n",
    "\n",
    "1. Tom M. Mitchell, Machine Learning, India Edition 2013, McGraw Hill Education.\n",
    "\n",
    " \n",
    "\n",
    "Reference Books:\n",
    "\n",
    "1. Trevor Hastie, Robert Tibshirani, Jerome Friedman, h The Elements of Statistical Learning, 2nd edition, springer series in statistics.\n",
    "\n",
    "2. Ethem Alpaydın, Introduction to machine learning, second edition, MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
